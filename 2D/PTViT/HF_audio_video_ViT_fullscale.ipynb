{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IY4PsvJq4oPl",
    "outputId": "f49e317c-36fe-42dd-c55a-92188c4344c3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fos4gtTU4qM5",
    "outputId": "f9b8f8e5-f434-4ab9-ca82-49c5dfd45c60"
   },
   "outputs": [],
   "source": [
    "# !python3 -m pip install einops\n",
    "# !python3 -m pip install facenet-pytorch\n",
    "# !python3 -m pip install face_alignment\n",
    "# !python3 -m pip install self_attention_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0UY1RHcZ4YNw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/riyaranj/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from einops.layers.torch import Rearrange\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from einops import rearrange\n",
    "from facenet_pytorch import MTCNN\n",
    "from transformers.modeling_outputs import ImageClassifierOutput  # Corrected import\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import face_alignment\n",
    "from transformers.models.vit.modeling_vit import ViTForImageClassification\n",
    "#import dlib\n",
    "import requests\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NiWEWAOF6b-S"
   },
   "outputs": [],
   "source": [
    "def extract_frame(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    mid_frame_index = frame_count // 2  # Index of the frame in the middle of the video\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_index)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cap.release()\n",
    "        return frame\n",
    "    else:\n",
    "        cap.release()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "l37ccKkO6cvt"
   },
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    mtcnn = MTCNN()\n",
    "    boxes, _ = mtcnn.detect(frame)\n",
    "    if boxes is not None:\n",
    "        # Assuming only one face in the frame\n",
    "        box = boxes[0]\n",
    "        x1, y1, x2, y2 = box\n",
    "        # Crop the frame to the detected face\n",
    "        cropped_frame = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "        return cropped_frame\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GUr4HOzW6fmC"
   },
   "outputs": [],
   "source": [
    "# Function to download the pretrained face alignment model if it doesn't exist\n",
    "def download_face_alignment_model(url, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        print(\"Downloading pretrained face alignment model...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download complete.\")\n",
    "\n",
    "# Specify the URL of the pretrained face alignment model\n",
    "face_alignment_model_url = \"https://github.com/1adrianb/face-alignment-models/releases/download/2.0.1/2DFAN4-11f355bf06.pth.tar\"\n",
    "\n",
    "# Download the pretrained face alignment model if it doesn't exist\n",
    "face_alignment_model_path = os.path.abspath(\"2DFAN4-11f355bf06.pth.tar\")\n",
    "download_face_alignment_model(face_alignment_model_url, face_alignment_model_path)\n",
    "\n",
    "# Initialize face alignment model\n",
    "fa = face_alignment.FaceAlignment(2, flip_input=False)  # 2 corresponds to 2D landmarks\n",
    "\n",
    "def align_face(frame):\n",
    "    # Perform face alignment\n",
    "    aligned_faces = fa.get_landmarks(frame)\n",
    "    if aligned_faces is not None:\n",
    "        aligned_face = aligned_faces[0]  # Assuming only one face in the frame\n",
    "        return aligned_face\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wYOMbbU975Fm"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(frame):\n",
    "    frame_pil = Image.fromarray(frame.astype('uint8'))\n",
    "    frame_pil = frame_pil.convert('RGB')  # Convert to RGB format\n",
    "    frame_np = np.array(frame_pil)\n",
    "    print(frame_np.shape)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize using ImageNet mean and std\n",
    "    ])\n",
    "    frame_tensor = transform(frame_pil)\n",
    "    return frame_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BkbbyV3a-Bax"
   },
   "outputs": [],
   "source": [
    "def preprocess_spectrogram(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')  # Convert to grayscale\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to match ViT input size\n",
    "        transforms.ToTensor(),           # Convert to tensor\n",
    "    ])\n",
    "    img_tensor = transform(img)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TIqOKFzr6j0V"
   },
   "outputs": [],
   "source": [
    "def load_spectrogram_dataset(spectrograms_folder, skipped_files):\n",
    "    X = []\n",
    "    y = []\n",
    "    # List all files in the input folder\n",
    "    files = sorted(os.listdir(spectrograms_folder))\n",
    "    # Iterate over files in the folder\n",
    "    for filename in tqdm(files):\n",
    "        if filename.endswith(\".png\") and filename[:-3] not in skipped_files:  # Assuming mel spectrograms are stored as PNG files\n",
    "            input_path = os.path.join(spectrograms_folder, filename)\n",
    "            img_tensor = preprocess_spectrogram(input_path)\n",
    "            X.append(img_tensor)\n",
    "            # Extract label from filename (assuming filename is in format \"abc_IEO_label_xyz.png\")\n",
    "            label = filename.split(\"_\")[2]\n",
    "            if label == \"HAP\":\n",
    "                y.append(0)\n",
    "            elif label == \"SAD\":\n",
    "                y.append(1)\n",
    "            elif label == \"ANG\":\n",
    "                y.append(2)\n",
    "            elif label == \"DIS\":\n",
    "                y.append(3)\n",
    "            elif label == \"FEA\":\n",
    "                y.append(4)\n",
    "            elif label == \"NEU\":\n",
    "                y.append(5)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "n4Xum7nf6lfg"
   },
   "outputs": [],
   "source": [
    "def load_dataset(videos_folder):\n",
    "    X = []\n",
    "    y = []\n",
    "    skipped_files = []\n",
    "    video_files = [file for file in sorted(os.listdir(videos_folder)) if file.endswith(\".flv\")]\n",
    "    for video_file in tqdm(video_files):\n",
    "        video_path = os.path.join(videos_folder, video_file)\n",
    "        frame = extract_frame(video_path)\n",
    "        if frame is not None:\n",
    "            cropped_face = detect_face(frame)\n",
    "            if cropped_face is not None:\n",
    "                preprocessed_face = preprocess_image(cropped_face)\n",
    "                X.append(preprocessed_face)\n",
    "                label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n",
    "                if label == \"HAP\":\n",
    "                    y.append(0)\n",
    "                elif label == \"SAD\":\n",
    "                    y.append(1)\n",
    "                elif label == \"ANG\":\n",
    "                    y.append(2)\n",
    "                elif label == \"DIS\":\n",
    "                    y.append(3)\n",
    "                elif label == \"FEA\":\n",
    "                    y.append(4)\n",
    "                elif label == \"NEU\":\n",
    "                    y.append(5)\n",
    "            else:\n",
    "                print(f\"No face detected in {video_file}. Skipping.\")\n",
    "                skipped_files.append(video_file[:-3])\n",
    "        else:\n",
    "            print(f\"Failed to extract frame from {video_file}. Skipping.\")\n",
    "            skipped_files.append(video_file[:-3])\n",
    "    return X, y, skipped_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q8gd6HoS--Rj"
   },
   "outputs": [],
   "source": [
    "# Define the ConcatDataset class to concatenate video frame and spectrogram tensors\n",
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X1, X2, y, modality='multimodal', fullscale=False, presaved=False, target_size=(224,224)):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.y = y\n",
    "        self.modality = modality\n",
    "        self.presaved = presaved\n",
    "        self.fullscale = fullscale\n",
    "        self.target_size = target_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.modality == 'audio':\n",
    "          return len(self.X2)\n",
    "        if self.modality == 'visual':\n",
    "          return len(self.X1)\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.fullscale:\n",
    "          img1 = self.X1[idx]\n",
    "          img2 = self.X2[idx]\n",
    "          label = self.y[idx]\n",
    "        else:\n",
    "          img1 = torch.from_numpy(self.X1[idx]).float()  # Convert numpy array to torch tensor\n",
    "          img2 = torch.from_numpy(self.X2[idx]).float()  # Convert numpy array to torch tensor\n",
    "          label = torch.tensor(self.y[idx])  # Convert numpy array to torch tensor\n",
    "\n",
    "        concatenated_img = torch.cat((img1, img2), dim=1)  # Concatenate along 1 dimension\n",
    "        concatenated_img = F.resize(concatenated_img, self.target_size)\n",
    "        #print(concatenated_img.shape)\n",
    "        \n",
    "        if self.modality == 'visual':\n",
    "          return img1, label\n",
    "        if self.modality == 'audio':\n",
    "          return img2, label\n",
    "        return concatenated_img, label # concatenate modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3TBMdqtI6oZ0"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Extract logits if the model output is wrapped in an ImageClassifierOutput object\n",
    "        if isinstance(outputs, ImageClassifierOutput):\n",
    "            logits = outputs.logits\n",
    "        else:\n",
    "            # Handle other cases\n",
    "            pass\n",
    "        \n",
    "        # Print the size of the logits tensor\n",
    "        #print(\"Logits size:\", logits.size())\n",
    "        \n",
    "        labels = labels.long()  # Convert labels to long type for CrossEntropyLoss\n",
    "        loss = criterion(logits, labels)  # Calculate loss using entire logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(train_loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aq6skVDB6p6M"
   },
   "outputs": [],
   "source": [
    "def test_model(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs.logits, labels)  # Calculate loss using entire logits\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predicted = torch.argmax(outputs.logits, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    accuracy = correct_preds / total_preds\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mhjpuUMgo3KU"
   },
   "outputs": [],
   "source": [
    "# class ViT(nn.Module):\n",
    "#     # ViT architecture adapted from here - https://theaisummer.com/vision-transformer/\n",
    "#     def __init__(self, *,\n",
    "#                  img_dim,\n",
    "#                  in_channels=3,\n",
    "#                  patch_dim=16,\n",
    "#                  num_classes=6, # full-scale CREMA-D\n",
    "#                  dim=512,\n",
    "#                  blocks=6,\n",
    "#                  heads=4,\n",
    "#                  dim_linear_block=1024,\n",
    "#                  dim_head=None,\n",
    "#                  dropout=0, transformer=None, classification=True):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             img_dim: the spatial image size\n",
    "#             in_channels: number of img channels\n",
    "#             patch_dim: desired patch dim\n",
    "#             num_classes: classification task classes\n",
    "#             dim: the linear layer's dim to project the patches for MHSA\n",
    "#             blocks: number of transformer blocks\n",
    "#             heads: number of heads\n",
    "#             dim_linear_block: inner dim of the transformer linear block\n",
    "#             dim_head: dim head in case you want to define it. defaults to dim/heads\n",
    "#             dropout: for pos emb and transformer\n",
    "#             transformer: in case you want to provide another transformer implementation\n",
    "#             classification: creates an extra CLS token\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         assert img_dim % patch_dim == 0, f'patch size {patch_dim} not divisible'\n",
    "#         self.p = patch_dim\n",
    "#         self.classification = classification\n",
    "#         tokens = (img_dim // patch_dim) ** 2\n",
    "#         self.token_dim = in_channels * (patch_dim ** 2)\n",
    "#         self.dim = dim\n",
    "#         self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
    "#         self.project_patches = nn.Linear(self.token_dim, dim)\n",
    "\n",
    "#         self.emb_dropout = nn.Dropout(dropout)\n",
    "#         if self.classification:\n",
    "#             self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "#             self.pos_emb1D = nn.Parameter(torch.randn(tokens + 1, dim))\n",
    "#             self.mlp_head = nn.Linear(dim, num_classes)\n",
    "#         else:\n",
    "#             self.pos_emb1D = nn.Parameter(torch.randn(tokens, dim))\n",
    "\n",
    "#         if transformer is None:\n",
    "#             self.transformer = TransformerEncoder(dim, blocks=blocks, heads=heads,\n",
    "#                                                   dim_head=self.dim_head,\n",
    "#                                                   dim_linear_block=dim_linear_block,\n",
    "#                                                   dropout=dropout)\n",
    "#         else:\n",
    "#             self.transformer = transformer\n",
    "\n",
    "#     def expand_cls_to_batch(self, batch):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             batch: batch size\n",
    "#         Returns: cls token expanded to the batch size\n",
    "#         \"\"\"\n",
    "#         return self.cls_token.expand([batch, -1, -1])\n",
    "\n",
    "#     def forward(self, img, mask=None):\n",
    "#         batch_size = img.shape[0]\n",
    "#         img_patches = rearrange(\n",
    "#         img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "#         patch_x=self.p, patch_y=self.p\n",
    "#     )\n",
    "#     # project patches with linear layer + add pos emb\n",
    "#         img_patches = self.project_patches(img_patches)\n",
    "\n",
    "#         if self.classification:\n",
    "#             img_patches = torch.cat(\n",
    "#             (self.expand_cls_to_batch(batch_size), img_patches), dim=1\n",
    "#         )\n",
    "\n",
    "#         patch_embeddings = self.emb_dropout(img_patches + self.pos_emb1D)\n",
    "\n",
    "#     # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n",
    "#         y = self.transformer(patch_embeddings, mask)\n",
    "\n",
    "#         if self.classification:\n",
    "#         # we index only the cls token for classification.\n",
    "#             return self.mlp_head(y[:, 0, :])\n",
    "#         else:\n",
    "#             return y  # Return logits directly\n",
    "#     # def forward(self, img, mask=None):\n",
    "#     #     batch_size = img.shape[0]\n",
    "#     #     img_patches = rearrange(\n",
    "#     #         img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "#     #                             patch_x=self.p, patch_y=self.p)\n",
    "#     #     # project patches with linear layer + add pos emb\n",
    "#     #     img_patches = self.project_patches(img_patches)\n",
    "\n",
    "#     #     if self.classification:\n",
    "#     #         img_patches = torch.cat(\n",
    "#     #             (self.expand_cls_to_batch(batch_size), img_patches), dim=1)\n",
    "\n",
    "#     #     patch_embeddings = self.emb_dropout(img_patches + self.pos_emb1D)\n",
    "\n",
    "#     #     # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n",
    "#     #     y = self.transformer(patch_embeddings, mask)\n",
    "\n",
    "#     #     if self.classification:\n",
    "#     #         # we index only the cls token for classification.\n",
    "#     #         return self.mlp_head(y[:, 0, :])\n",
    "#     #     else:\n",
    "#     #         return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fullscale = True # Run fullscale experiment?\n",
    "_presaved = True # Use presaved data .npy files?\n",
    "\n",
    "# Define input_folder and input_folder_spec\n",
    "if _fullscale:\n",
    "  input_folder = '/home1/riyaranj/riya/videos_fullscale'\n",
    "  input_folder_spec = '/home1/riyaranj/riya/melspec_fullscale'\n",
    "else:\n",
    "  input_folder = '/home1/riyaranj/riya/videos'\n",
    "  input_folder_spec = '/home1/riyaranj/riya/melspec'\n",
    "\n",
    "# Check if input folder exists\n",
    "if not os.path.exists(input_folder):\n",
    "    print(\"Input folder does not exist.\")\n",
    "    sys.exit(1)\n",
    "# Check if input folder exists\n",
    "if not os.path.exists(input_folder_spec):\n",
    "    print(\"Input folder does not exist.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that the videos and spectrograms are in one-to-one correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert([file[:-3] for file in sorted(os.listdir(input_folder))] == [file[:-3] for file in sorted(os.listdir(input_folder_spec))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAHGApwK4N92",
    "outputId": "b29916d3-9aa5-47e3-87a0-de2dcf4c24ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 7429\n",
      "Number of train samples (video): 5200 Number of test samples: 2229\n",
      "Number of train samples (audio): 5200 Number of test samples: 2229\n"
     ]
    }
   ],
   "source": [
    "if _presaved:\n",
    "  X = np.load('/home1/riyaranj/riya/X_7439.npy', mmap_mode='r')\n",
    "  y = np.load('/home1/riyaranj/riya/y_7439.npy', mmap_mode='r')\n",
    "  X_spec = np.load('/home1/riyaranj/riya/X_spec_7439.npy', mmap_mode='r')\n",
    "  y_spec = np.load('/home1/riyaranj/riya/y_spec_7439.npy', mmap_mode='r')\n",
    "else:\n",
    "    X, y, skipped_files = load_dataset(input_folder)\n",
    "    X_spec, y_spec = load_spectrogram_dataset(input_folder_spec, skipped_files)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "print(f\"Total number of samples: {len(X)}\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"Number of train samples (video): {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n",
    "X_train_spec, X_test_spec, y_train_spec, y_test_spec = train_test_split(X_spec, y_spec, test_size=0.3, random_state=42)\n",
    "print(f\"Number of train samples (audio): {len(X_train_spec)}\", f\"Number of test samples: {len(X_test_spec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data as .npy files (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save X, y, X_spec, y_spec\n",
    "# np.save('X_7439.npy', np.array(X))\n",
    "# np.save('y_7439.npy', np.array(y))\n",
    "# np.save('X_spec_7439.npy', np.array(X_spec))\n",
    "# np.save('y_spec_7439.npy', np.array(y_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that the number of frames and spectrograms are equal and that the train + test samples add up to total samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X_train) == len(X_train_spec) and len(X_test) == len(X_test_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wzuHjENY9Dpa"
   },
   "outputs": [],
   "source": [
    "def train_ViT(_modality):\n",
    "    best_test_accuracy = 0.0\n",
    "    best_results = None\n",
    "    best_model_state = None\n",
    "\n",
    "    batch_sizes = [32]\n",
    "    learning_rates = [0.0001]\n",
    "\n",
    "    for _bs in batch_sizes:\n",
    "        for _lr in learning_rates:\n",
    "            # Adjust input channels as per modality\n",
    "            if _modality == 'multimodal':\n",
    "                _input_channels = 2\n",
    "            else:\n",
    "                _input_channels = 1\n",
    "\n",
    "            # Initialize the ViT model\n",
    "            config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "            config.num_labels = 6\n",
    "            model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "            # Define device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "\n",
    "            # Define loss function and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
    "\n",
    "            # Concatenate datasets if multimodal\n",
    "            train_dataset = ConcatDataset(X_train, X_train_spec, y_train, modality=_modality, fullscale = _fullscale)\n",
    "            test_dataset = ConcatDataset(X_test, X_test_spec, y_test, modality=_modality, fullscale = _fullscale)\n",
    "\n",
    "            # Create data loaders\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=_bs, shuffle=True)\n",
    "            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=_bs)\n",
    "\n",
    "            print(f\"\\n\\nBatch size: {_bs}, lr: {_lr}\")\n",
    "\n",
    "            # Training loop\n",
    "            num_epochs = 30\n",
    "            print(f\"Training ViT for \\\"{_modality}\\\" pipeline ...\\n------------------------------------------------\\n\")\n",
    "            for epoch in range(num_epochs):\n",
    "                print(\"Epoch \" + str(epoch))\n",
    "                train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
    "                test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "                # Update best model and results if test accuracy improves\n",
    "                if test_accuracy > best_test_accuracy:\n",
    "                    best_test_accuracy = test_accuracy\n",
    "                    best_results = (train_loss, train_accuracy, test_loss, test_accuracy, _bs, _lr)\n",
    "                    best_model_state = model.state_dict()\n",
    "\n",
    "            # Save the model\n",
    "            if _modality == 'multimodal':\n",
    "                torch.save(model.state_dict(), f'HF_ViT_audio_video_fullscale_DIM1_{num_epochs}_{_bs}_{_lr}.pt')\n",
    "            elif _modality == 'audio':\n",
    "                torch.save(model.state_dict(), f'HF_ViT_audio_fullscale_DIM1_{num_epochs}_{_bs}_{_lr}.pt')\n",
    "            elif _modality == 'visual':\n",
    "                torch.save(model.state_dict(), f'HF_ViT_video_fullscale_DIM1_{num_epochs}_{_bs}_{_lr}.pt')\n",
    "            else:\n",
    "                print(\"Improper modality provided!\")\n",
    "\n",
    "    print(\"Best Results:\")\n",
    "    print(f\"Train Loss: {best_results[0]}, Train Accuracy: {best_results[1]}, Test Loss: {best_results[2]}, Test Accuracy: {best_results[3]}, Batch Size: {best_results[4]}, Learning Rate: {best_results[5]}\")\n",
    "\n",
    "    return best_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CXy5mkXruLRr"
   },
   "outputs": [],
   "source": [
    "# # Define modalities\n",
    "_modality = ['visual', 'audio', 'multimodal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "eaq2Nfvs9IZm",
    "outputId": "1ad0ef17-01f3-41d8-8804-93960060b814"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Batch size: 32, lr: 0.0001\n",
      "Training ViT for \"visual\" pipeline ...\n",
      "------------------------------------------------\n",
      "\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 1.3392, Train Accuracy: 0.4663, Test Loss: 1.0887, Test Accuracy: 0.5617\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Train Loss: 0.9266, Train Accuracy: 0.6542, Test Loss: 0.9846, Test Accuracy: 0.6223\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Train Loss: 0.6780, Train Accuracy: 0.7560, Test Loss: 1.0062, Test Accuracy: 0.6335\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Train Loss: 0.4935, Train Accuracy: 0.8262, Test Loss: 0.9056, Test Accuracy: 0.6685\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Train Loss: 0.3065, Train Accuracy: 0.9037, Test Loss: 0.9413, Test Accuracy: 0.6886\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 0.2363, Train Accuracy: 0.9219, Test Loss: 1.1432, Test Accuracy: 0.6546\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Train Loss: 0.1878, Train Accuracy: 0.9379, Test Loss: 1.1379, Test Accuracy: 0.6761\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Train Loss: 0.1585, Train Accuracy: 0.9471, Test Loss: 1.1409, Test Accuracy: 0.6783\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Train Loss: 0.0943, Train Accuracy: 0.9729, Test Loss: 1.1985, Test Accuracy: 0.6716\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Train Loss: 0.1045, Train Accuracy: 0.9679, Test Loss: 1.2460, Test Accuracy: 0.6765\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Train Loss: 0.0798, Train Accuracy: 0.9742, Test Loss: 1.3838, Test Accuracy: 0.6406\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Train Loss: 0.1018, Train Accuracy: 0.9667, Test Loss: 1.2686, Test Accuracy: 0.6761\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Train Loss: 0.0638, Train Accuracy: 0.9794, Test Loss: 1.3436, Test Accuracy: 0.6783\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Train Loss: 0.0758, Train Accuracy: 0.9756, Test Loss: 1.5786, Test Accuracy: 0.6276\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Train Loss: 0.0566, Train Accuracy: 0.9827, Test Loss: 1.4364, Test Accuracy: 0.6599\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Train Loss: 0.0454, Train Accuracy: 0.9858, Test Loss: 1.3825, Test Accuracy: 0.6895\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Train Loss: 0.0383, Train Accuracy: 0.9896, Test Loss: 1.3634, Test Accuracy: 0.6815\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Train Loss: 0.0631, Train Accuracy: 0.9800, Test Loss: 1.5137, Test Accuracy: 0.6590\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Train Loss: 0.0533, Train Accuracy: 0.9825, Test Loss: 1.4368, Test Accuracy: 0.6631\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Train Loss: 0.0430, Train Accuracy: 0.9858, Test Loss: 1.4104, Test Accuracy: 0.6738\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Train Loss: 0.0583, Train Accuracy: 0.9808, Test Loss: 1.4161, Test Accuracy: 0.6729\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Train Loss: 0.0477, Train Accuracy: 0.9842, Test Loss: 1.3953, Test Accuracy: 0.6725\n",
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Train Loss: 0.0368, Train Accuracy: 0.9892, Test Loss: 1.5929, Test Accuracy: 0.6631\n",
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Train Loss: 0.0461, Train Accuracy: 0.9838, Test Loss: 1.4749, Test Accuracy: 0.6756\n",
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Train Loss: 0.0292, Train Accuracy: 0.9912, Test Loss: 1.5896, Test Accuracy: 0.6712\n",
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Train Loss: 0.0373, Train Accuracy: 0.9906, Test Loss: 1.5287, Test Accuracy: 0.6496\n",
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Train Loss: 0.0366, Train Accuracy: 0.9881, Test Loss: 1.4479, Test Accuracy: 0.6703\n",
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Train Loss: 0.0549, Train Accuracy: 0.9835, Test Loss: 1.3764, Test Accuracy: 0.6756\n",
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Train Loss: 0.0385, Train Accuracy: 0.9879, Test Loss: 1.6881, Test Accuracy: 0.6492\n",
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Train Loss: 0.0277, Train Accuracy: 0.9917, Test Loss: 1.5532, Test Accuracy: 0.6716\n",
      "Best Results:\n",
      "Train Loss: 0.0453882498746635, Train Accuracy: 0.9857692307692307, Test Loss: 1.3825240892484725, Test Accuracy: 0.6895468820098699, Batch Size: 32, Learning Rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Batch size: 32, lr: 0.0001\n",
      "Training ViT for \"audio\" pipeline ...\n",
      "------------------------------------------------\n",
      "\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 1.4547, Train Accuracy: 0.4115, Test Loss: 1.3350, Test Accuracy: 0.4625\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Train Loss: 1.1529, Train Accuracy: 0.5635, Test Loss: 1.1059, Test Accuracy: 0.5765\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Train Loss: 0.9150, Train Accuracy: 0.6696, Test Loss: 1.0073, Test Accuracy: 0.6160\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Train Loss: 0.6783, Train Accuracy: 0.7619, Test Loss: 1.1671, Test Accuracy: 0.5980\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Train Loss: 0.4892, Train Accuracy: 0.8346, Test Loss: 1.2639, Test Accuracy: 0.5716\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 0.3420, Train Accuracy: 0.8887, Test Loss: 1.2520, Test Accuracy: 0.5935\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Train Loss: 0.2307, Train Accuracy: 0.9271, Test Loss: 1.3478, Test Accuracy: 0.6003\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Train Loss: 0.1738, Train Accuracy: 0.9471, Test Loss: 1.5334, Test Accuracy: 0.5900\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Train Loss: 0.1659, Train Accuracy: 0.9421, Test Loss: 1.4022, Test Accuracy: 0.6092\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Train Loss: 0.1097, Train Accuracy: 0.9654, Test Loss: 1.6525, Test Accuracy: 0.5922\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Train Loss: 0.0749, Train Accuracy: 0.9771, Test Loss: 1.6061, Test Accuracy: 0.6146\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Train Loss: 0.0777, Train Accuracy: 0.9773, Test Loss: 1.6785, Test Accuracy: 0.6074\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Train Loss: 0.0792, Train Accuracy: 0.9773, Test Loss: 1.6596, Test Accuracy: 0.6101\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Train Loss: 0.1118, Train Accuracy: 0.9629, Test Loss: 1.7572, Test Accuracy: 0.5882\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Train Loss: 0.0508, Train Accuracy: 0.9840, Test Loss: 1.7703, Test Accuracy: 0.6052\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Train Loss: 0.0546, Train Accuracy: 0.9846, Test Loss: 1.7709, Test Accuracy: 0.6231\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Train Loss: 0.0442, Train Accuracy: 0.9877, Test Loss: 1.9515, Test Accuracy: 0.5774\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Train Loss: 0.0739, Train Accuracy: 0.9752, Test Loss: 1.8832, Test Accuracy: 0.5742\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Train Loss: 0.0544, Train Accuracy: 0.9808, Test Loss: 1.7781, Test Accuracy: 0.6160\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Train Loss: 0.0194, Train Accuracy: 0.9948, Test Loss: 1.9284, Test Accuracy: 0.6079\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Train Loss: 0.0439, Train Accuracy: 0.9850, Test Loss: 1.8517, Test Accuracy: 0.5904\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Train Loss: 0.0572, Train Accuracy: 0.9825, Test Loss: 1.8834, Test Accuracy: 0.5891\n",
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Train Loss: 0.0520, Train Accuracy: 0.9831, Test Loss: 2.0403, Test Accuracy: 0.5787\n",
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Train Loss: 0.0463, Train Accuracy: 0.9844, Test Loss: 2.0163, Test Accuracy: 0.5935\n",
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Train Loss: 0.0537, Train Accuracy: 0.9829, Test Loss: 1.9283, Test Accuracy: 0.6070\n",
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Train Loss: 0.0178, Train Accuracy: 0.9954, Test Loss: 1.9243, Test Accuracy: 0.6097\n",
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Train Loss: 0.0313, Train Accuracy: 0.9908, Test Loss: 1.9755, Test Accuracy: 0.6061\n",
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Train Loss: 0.0857, Train Accuracy: 0.9729, Test Loss: 1.9046, Test Accuracy: 0.5877\n",
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Train Loss: 0.0213, Train Accuracy: 0.9935, Test Loss: 1.9126, Test Accuracy: 0.6178\n",
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Train Loss: 0.0137, Train Accuracy: 0.9960, Test Loss: 2.1105, Test Accuracy: 0.5967\n",
      "Best Results:\n",
      "Train Loss: 0.05455524961755502, Train Accuracy: 0.9846153846153847, Test Loss: 1.77093934943292, Test Accuracy: 0.6231493943472409, Batch Size: 32, Learning Rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Batch size: 32, lr: 0.0001\n",
      "Training ViT for \"multimodal\" pipeline ...\n",
      "------------------------------------------------\n",
      "\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 1.2925, Train Accuracy: 0.5013, Test Loss: 1.0592, Test Accuracy: 0.5976\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Train Loss: 0.7868, Train Accuracy: 0.7133, Test Loss: 0.7210, Test Accuracy: 0.7376\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Train Loss: 0.5444, Train Accuracy: 0.8108, Test Loss: 0.6691, Test Accuracy: 0.7649\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Train Loss: 0.3775, Train Accuracy: 0.8777, Test Loss: 0.6238, Test Accuracy: 0.7802\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Train Loss: 0.2442, Train Accuracy: 0.9256, Test Loss: 0.6792, Test Accuracy: 0.7721\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 0.1795, Train Accuracy: 0.9440, Test Loss: 0.6467, Test Accuracy: 0.7896\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Train Loss: 0.1467, Train Accuracy: 0.9496, Test Loss: 0.7191, Test Accuracy: 0.7761\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Train Loss: 0.0714, Train Accuracy: 0.9806, Test Loss: 0.8050, Test Accuracy: 0.7824\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Train Loss: 0.1168, Train Accuracy: 0.9604, Test Loss: 0.7795, Test Accuracy: 0.7945\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Train Loss: 0.1225, Train Accuracy: 0.9608, Test Loss: 0.7036, Test Accuracy: 0.8066\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Train Loss: 0.0560, Train Accuracy: 0.9829, Test Loss: 0.7585, Test Accuracy: 0.8008\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Train Loss: 0.0689, Train Accuracy: 0.9781, Test Loss: 0.6896, Test Accuracy: 0.8174\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Train Loss: 0.0542, Train Accuracy: 0.9829, Test Loss: 0.8426, Test Accuracy: 0.7824\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Train Loss: 0.0387, Train Accuracy: 0.9888, Test Loss: 0.9258, Test Accuracy: 0.7685\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Train Loss: 0.0393, Train Accuracy: 0.9894, Test Loss: 0.8212, Test Accuracy: 0.8107\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Train Loss: 0.0284, Train Accuracy: 0.9921, Test Loss: 0.9268, Test Accuracy: 0.7770\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Train Loss: 0.0691, Train Accuracy: 0.9788, Test Loss: 0.7669, Test Accuracy: 0.7963\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Train Loss: 0.0218, Train Accuracy: 0.9946, Test Loss: 0.8964, Test Accuracy: 0.7918\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Train Loss: 0.0693, Train Accuracy: 0.9769, Test Loss: 0.8952, Test Accuracy: 0.7730\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Train Loss: 0.0397, Train Accuracy: 0.9869, Test Loss: 0.8376, Test Accuracy: 0.7986\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Train Loss: 0.0398, Train Accuracy: 0.9867, Test Loss: 0.7543, Test Accuracy: 0.8291\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Train Loss: 0.0294, Train Accuracy: 0.9906, Test Loss: 0.8186, Test Accuracy: 0.7950\n",
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Train Loss: 0.0471, Train Accuracy: 0.9844, Test Loss: 0.9005, Test Accuracy: 0.8017\n",
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Train Loss: 0.0342, Train Accuracy: 0.9904, Test Loss: 0.8930, Test Accuracy: 0.7954\n",
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Train Loss: 0.0433, Train Accuracy: 0.9852, Test Loss: 0.8898, Test Accuracy: 0.7932\n",
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Train Loss: 0.0191, Train Accuracy: 0.9952, Test Loss: 0.8928, Test Accuracy: 0.7954\n",
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Train Loss: 0.0250, Train Accuracy: 0.9919, Test Loss: 0.9407, Test Accuracy: 0.7820\n",
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Train Loss: 0.0254, Train Accuracy: 0.9913, Test Loss: 0.8325, Test Accuracy: 0.7963\n",
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Train Loss: 0.0193, Train Accuracy: 0.9933, Test Loss: 0.9226, Test Accuracy: 0.8008\n",
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Train Loss: 0.0337, Train Accuracy: 0.9890, Test Loss: 1.0044, Test Accuracy: 0.7707\n",
      "Best Results:\n",
      "Train Loss: 0.03979573138715253, Train Accuracy: 0.9867307692307692, Test Loss: 0.7543248275621636, Test Accuracy: 0.82907133243607, Batch Size: 32, Learning Rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train ViT\n",
    "for _m in _modality:\n",
    "    train_ViT(_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12.2 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
