{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IY4PsvJq4oPl",
    "outputId": "54dae53e-7b2c-457b-cf5e-306d15b11592"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fos4gtTU4qM5",
    "outputId": "38f0e6b1-55b4-4831-a009-96deb7584ae0"
   },
   "outputs": [],
   "source": [
    "# !python3 -m pip install einops\n",
    "# !python3 -m pip install facenet-pytorch\n",
    "# !python3 -m pip install face_alignment\n",
    "# !python3 -m pip install self_attention_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7439\n"
     ]
    }
   ],
   "source": [
    "!ls 'Videos7439' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7439\n"
     ]
    }
   ],
   "source": [
    "!ls 'Spectrograms7439' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_folder = 'Videos7439'\n",
    "spectrograms_folder = 'Spectrograms7439'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0UY1RHcZ4YNw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/riyaranj/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange\n",
    "from facenet_pytorch import MTCNN\n",
    "from self_attention_cv import TransformerEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import face_alignment\n",
    "#import dlib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NiWEWAOF6b-S"
   },
   "outputs": [],
   "source": [
    "def extract_frame(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    mid_frame_index = frame_count // 2  # Index of the frame in the middle of the video\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_index)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cap.release()\n",
    "        return frame\n",
    "    else:\n",
    "        cap.release()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "l37ccKkO6cvt"
   },
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    mtcnn = MTCNN()\n",
    "    boxes, _ = mtcnn.detect(frame)\n",
    "    if boxes is not None:\n",
    "        # Assuming only one face in the frame\n",
    "        box = boxes[0]\n",
    "        x1, y1, x2, y2 = box\n",
    "        # Crop the frame to the detected face\n",
    "        cropped_frame = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "        return cropped_frame\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUr4HOzW6fmC",
    "outputId": "0cc73263-b8b4-4860-bd59-f430372070a5"
   },
   "outputs": [],
   "source": [
    "# Function to download the pretrained face alignment model if it doesn't exist\n",
    "def download_face_alignment_model(url, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        print(\"Downloading pretrained face alignment model...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download complete.\")\n",
    "\n",
    "# Specify the URL of the pretrained face alignment model\n",
    "face_alignment_model_url = \"https://github.com/1adrianb/face-alignment-models/releases/download/2.0.1/2DFAN4-11f355bf06.pth.tar\"\n",
    "\n",
    "# Download the pretrained face alignment model if it doesn't exist\n",
    "face_alignment_model_path = os.path.abspath(\"2DFAN4-11f355bf06.pth.tar\")\n",
    "download_face_alignment_model(face_alignment_model_url, face_alignment_model_path)\n",
    "\n",
    "# Initialize face alignment model\n",
    "fa = face_alignment.FaceAlignment(2, flip_input=False)  # 2 corresponds to 2D landmarks\n",
    "\n",
    "def align_face(frame):\n",
    "    # Perform face alignment\n",
    "    aligned_faces = fa.get_landmarks(frame)\n",
    "    if aligned_faces is not None:\n",
    "        aligned_face = aligned_faces[0]  # Assuming only one face in the frame\n",
    "        return aligned_face\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wYOMbbU975Fm"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the preprocessing functions for video frames and spectrograms\n",
    "def preprocess_image(frame):\n",
    "    frame_pil = Image.fromarray(frame.astype('uint8'))\n",
    "    frame_pil = frame_pil.convert('L')  # Convert to grayscale\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229]),  # Normalize using ImageNet mean and std\n",
    "    ])\n",
    "    frame_tensor = transform(frame_pil)\n",
    "    return frame_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BkbbyV3a-Bax"
   },
   "outputs": [],
   "source": [
    "def preprocess_spectrogram(image_path):\n",
    "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to match ViT input size\n",
    "        transforms.ToTensor(),           # Convert to tensor\n",
    "    ])\n",
    "    img_tensor = transform(img)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TIqOKFzr6j0V"
   },
   "outputs": [],
   "source": [
    "def load_spectrogram_dataset(spectrograms_folder, skipped_files):\n",
    "    X = []\n",
    "    y = []\n",
    "    # List all files in the input folder\n",
    "    files = sorted(os.listdir(spectrograms_folder))\n",
    "    # Iterate over files in the folder\n",
    "    for filename in tqdm(files):\n",
    "        if filename.endswith(\".png\") and filename[:-3] not in skipped_files:  # Assuming mel spectrograms are stored as PNG files\n",
    "            input_path = os.path.join(spectrograms_folder, filename)\n",
    "            img_tensor = preprocess_spectrogram(input_path)\n",
    "            X.append(img_tensor)\n",
    "            # Extract label from filename (assuming filename is in format \"abc_IEO_label_xyz.png\")\n",
    "            label = filename.split(\"_\")[2]\n",
    "            if label == \"HAP\":\n",
    "                y.append(0)\n",
    "            elif label == \"SAD\":\n",
    "                y.append(1)\n",
    "            elif label == \"ANG\":\n",
    "                y.append(2)\n",
    "            elif label == \"DIS\":\n",
    "                y.append(3)\n",
    "            elif label == \"FEA\":\n",
    "                y.append(4)\n",
    "            elif label == \"NEU\":\n",
    "                y.append(5)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "n4Xum7nf6lfg"
   },
   "outputs": [],
   "source": [
    "def load_dataset(videos_folder):\n",
    "    X = []\n",
    "    y = []\n",
    "    skipped_files = []\n",
    "    video_files = [file for file in sorted(os.listdir(videos_folder)) if file.endswith(\".flv\")]\n",
    "    for video_file in tqdm(video_files):\n",
    "        video_path = os.path.join(videos_folder, video_file)\n",
    "        frame = extract_frame(video_path)\n",
    "        if frame is not None:\n",
    "            cropped_face = detect_face(frame)\n",
    "            if cropped_face is not None:\n",
    "                preprocessed_face = preprocess_image(cropped_face)\n",
    "                X.append(preprocessed_face)\n",
    "                label = video_file.split(\"_\")[2].split(\".\")[0]  # Adjusted to handle different file extensions\n",
    "                if label == \"HAP\":\n",
    "                    y.append(0)\n",
    "                elif label == \"SAD\":\n",
    "                    y.append(1)\n",
    "                elif label == \"ANG\":\n",
    "                    y.append(2)\n",
    "                elif label == \"DIS\":\n",
    "                    y.append(3)\n",
    "                elif label == \"FEA\":\n",
    "                    y.append(4)\n",
    "                elif label == \"NEU\":\n",
    "                    y.append(5)\n",
    "            else:\n",
    "                print(f\"No face detected in {video_file}. Skipping.\")\n",
    "                skipped_files.append(video_file[:-3])\n",
    "        else:\n",
    "            print(f\"Failed to extract frame from {video_file}. Skipping.\")\n",
    "            skipped_files.append(video_file[:-3])\n",
    "    return X, y, skipped_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "q8gd6HoS--Rj"
   },
   "outputs": [],
   "source": [
    "# Define the ConcatDataset class to concatenate video frame and spectrogram tensors\n",
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X1, X2, y, modality='multimodal', presaved=False):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.y = y\n",
    "        self.modality = modality\n",
    "        self.presaved = presaved\n",
    "    def __len__(self):\n",
    "        if self.modality == 'audio':\n",
    "          return len(self.X2)\n",
    "        if self.modality == 'visual':\n",
    "          return len(self.X1)\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.presaved:\n",
    "          img1 = self.X1[idx]\n",
    "          img2 = self.X2[idx]\n",
    "          label = self.y[idx]\n",
    "        else:\n",
    "          img1 = torch.from_numpy(self.X1[idx]).float()  # Convert numpy array to torch tensor\n",
    "          img2 = torch.from_numpy(self.X2[idx]).float()  # Convert numpy array to torch tensor\n",
    "          label = torch.tensor(self.y[idx])  # Convert numpy array to torch tensor\n",
    "\n",
    "        concatenated_img = torch.cat((img1, img2), dim=0)  # Concatenate along 0 dimension\n",
    "        if self.modality == 'visual':\n",
    "          return img1, label\n",
    "        if self.modality == 'audio':\n",
    "          return img2, label\n",
    "        return concatenated_img, label # concatenate modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3TBMdqtI6oZ0"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct_preds / total_preds\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aq6skVDB6p6M"
   },
   "outputs": [],
   "source": [
    "def test_model(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    accuracy = correct_preds / total_preds\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mhjpuUMgo3KU"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    # ViT architecture adapted from here - https://theaisummer.com/vision-transformer/\n",
    "    def __init__(self, *,\n",
    "                 img_dim,\n",
    "                 in_channels=3,\n",
    "                 patch_dim=16,\n",
    "                 num_classes=6, # full-scale CREMA-D\n",
    "                 dim=512,\n",
    "                 blocks=6,\n",
    "                 heads=4,\n",
    "                 dim_linear_block=1024,\n",
    "                 dim_head=None,\n",
    "                 dropout=0.4, transformer=None, classification=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dim: the spatial image size\n",
    "            in_channels: number of img channels\n",
    "            patch_dim: desired patch dim\n",
    "            num_classes: classification task classes\n",
    "            dim: the linear layer's dim to project the patches for MHSA\n",
    "            blocks: number of transformer blocks\n",
    "            heads: number of heads\n",
    "            dim_linear_block: inner dim of the transformer linear block\n",
    "            dim_head: dim head in case you want to define it. defaults to dim/heads\n",
    "            dropout: for pos emb and transformer\n",
    "            transformer: in case you want to provide another transformer implementation\n",
    "            classification: creates an extra CLS token\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert img_dim % patch_dim == 0, f'patch size {patch_dim} not divisible'\n",
    "        self.p = patch_dim\n",
    "        self.classification = classification\n",
    "        tokens = (img_dim // patch_dim) ** 2\n",
    "        self.token_dim = in_channels * (patch_dim ** 2)\n",
    "        self.dim = dim\n",
    "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
    "        self.project_patches = nn.Linear(self.token_dim, dim)\n",
    "\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "        if self.classification:\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "            self.pos_emb1D = nn.Parameter(torch.randn(tokens + 1, dim))\n",
    "            self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        else:\n",
    "            self.pos_emb1D = nn.Parameter(torch.randn(tokens, dim))\n",
    "\n",
    "        if transformer is None:\n",
    "            self.transformer = TransformerEncoder(dim, blocks=blocks, heads=heads,\n",
    "                                                  dim_head=self.dim_head,\n",
    "                                                  dim_linear_block=dim_linear_block,\n",
    "                                                  dropout=dropout)\n",
    "        else:\n",
    "            self.transformer = transformer\n",
    "\n",
    "    def expand_cls_to_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: batch size\n",
    "        Returns: cls token expanded to the batch size\n",
    "        \"\"\"\n",
    "        return self.cls_token.expand([batch, -1, -1])\n",
    "\n",
    "    def forward(self, img, mask=None):\n",
    "        batch_size = img.shape[0]\n",
    "        img_patches = rearrange(\n",
    "            img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "                                patch_x=self.p, patch_y=self.p)\n",
    "        # project patches with linear layer + add pos emb\n",
    "        img_patches = self.project_patches(img_patches)\n",
    "\n",
    "        if self.classification:\n",
    "            img_patches = torch.cat(\n",
    "                (self.expand_cls_to_batch(batch_size), img_patches), dim=1)\n",
    "\n",
    "        patch_embeddings = self.emb_dropout(img_patches + self.pos_emb1D)\n",
    "\n",
    "        # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n",
    "        y = self.transformer(patch_embeddings, mask)\n",
    "\n",
    "        if self.classification:\n",
    "            # we index only the cls token for classification.\n",
    "            return self.mlp_head(y[:, 0, :])\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert([file[:-3] for file in sorted(os.listdir(videos_folder))] == [file[:-3] for file in sorted(os.listdir(spectrograms_folder))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAHGApwK4N92",
    "outputId": "bbc7a21f-8101-4d65-9a62-ea8e71209ef0"
   },
   "outputs": [],
   "source": [
    "_fullscale = True # Run fullscale experiment?\n",
    "_presaved = True # Use presaved data .npy files?\n",
    "\n",
    "# Check if data folders exists\n",
    "if not os.path.exists(videos_folder):\n",
    "    print(\"Videos folder does not exist.\")\n",
    "    sys.exit(1)\n",
    "if not os.path.exists(spectrograms_folder):\n",
    "    print(\"Spectrograms folder does not exist.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Load dataset and split into train and test sets\n",
    "if _presaved:\n",
    "  X = np.load('/home1/riyaranj/riya/X_7439.npy', mmap_mode='r')\n",
    "  y = np.load('/home1/riyaranj/riya/y_7439.npy', mmap_mode='r')\n",
    "  X_spec = np.load('/home1/riyaranj/riya/X_spec_7439.npy', mmap_mode='r')\n",
    "  y_spec = np.load('/home1/riyaranj/riya/y_spec_7439.npy', mmap_mode='r')\n",
    "else:\n",
    "  X, y, skipped_files = load_dataset(videos_folder)\n",
    "  X_spec, y_spec = load_spectrogram_dataset(spectrograms_folder, skipped_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3gzXGfQPSBAC"
   },
   "outputs": [],
   "source": [
    "# # Save X, y, X_spec, y_spec\n",
    "# np.save('X_7439.npy', np.array(X))\n",
    "# np.save('y_7439.npy', np.array(y))\n",
    "# np.save('X_spec_7439.npy', np.array(X_spec))\n",
    "# np.save('y_spec_7439.npy', np.array(y_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 7429\n",
      "Number of train samples (video): 5200 Number of test samples: 2229\n",
      "Number of train samples (audio): 5200 Number of test samples: 2229\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "print(f\"Total number of samples: {len(X)}\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"Number of train samples (video): {len(X_train)}\", f\"Number of test samples: {len(X_test)}\")\n",
    "X_train_spec, X_test_spec, y_train_spec, y_test_spec = train_test_split(X_spec, y_spec, test_size=0.3, random_state=42)\n",
    "print(f\"Number of train samples (audio): {len(X_train_spec)}\", f\"Number of test samples: {len(X_test_spec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X_train) == len(X_train_spec) and len(X_test) == len(X_test_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wzuHjENY9Dpa"
   },
   "outputs": [],
   "source": [
    "def train_ViT(_modality):\n",
    "  # Adjust input channels as per modality\n",
    "  if _modality == 'multimodal':\n",
    "    _input_channels = 2\n",
    "  else:\n",
    "    _input_channels = 1\n",
    "\n",
    "  # Initialize the ViT model\n",
    "  model = ViT(img_dim=224,  # Image dimension\n",
    "              in_channels=_input_channels,  # Number of input channels\n",
    "              patch_dim=16,  # Patch dimension\n",
    "              num_classes=6,  # 6 classes for HAPPY, SAD, ANGRY, DISGUST, FEAR, NEUTRAL\n",
    "              dim=768,  # Dimensionality of the token embeddings\n",
    "              blocks=16,  # Number of transformer blocks\n",
    "              heads=16,  # Number of attention heads\n",
    "              dim_linear_block=1024,  # Dimensionality of the linear block\n",
    "              dropout=0.3,  # Dropout rate\n",
    "              classification=True)  # Whether or not to include a classification token\n",
    "\n",
    "  # Define device\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model.to(device)\n",
    "\n",
    "  # Define loss function and optimizer\n",
    "  _lr = 0.0001\n",
    "  heads = 16\n",
    "  blocks = 16\n",
    "  dr = 0.3\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
    "\n",
    "  # Concatenate datasets if multimodal\n",
    "  train_dataset = ConcatDataset(X_train, X_train_spec, y_train, modality = _modality, presaved = _presaved)\n",
    "  test_dataset = ConcatDataset(X_test, X_test_spec, y_test, modality = _modality, presaved = _presaved)\n",
    "\n",
    "  # Create data loaders\n",
    "  _bs = 64\n",
    "\n",
    "  # Create data loaders\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=_bs, shuffle=True)\n",
    "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=_bs)\n",
    "\n",
    "  print(f\"\\n\\nBatch size: {_bs}\", f\"lr: {_lr}\")\n",
    "\n",
    "  # Training loop\n",
    "  num_epochs = 50\n",
    "  print(f\"Training ViT for \\\"{_modality}\\\" pipeline ...\\n------------------------------------------------\\n\")\n",
    "  for epoch in range(num_epochs):\n",
    "      print(\"Epoch \" + str(epoch))\n",
    "      train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
    "      test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "  # Save the model\n",
    "  if _modality == 'multimodal':\n",
    "    torch.save(model.state_dict(), 'ViT_audio_video_fullscale_'+'_'+str(_bs)+'_'+str(heads)+'_'+str(blocks)+'_'+str(dr)+'_'+str(_lr))\n",
    "  elif _modality == 'audio':\n",
    "    torch.save(model.state_dict(), 'ViT_audio_fullscale_'+'_'+str(_bs)+'_'+str(heads)+'_'+str(blocks)+'_'+str(dr)+'_'+str(_lr))\n",
    "  elif _modality == 'visual':\n",
    "    torch.save(model.state_dict(), 'ViT_video_fullscale_'+'_'+str(_bs)+'_'+str(heads)+'_'+str(blocks)+'_'+str(dr)+'_'+str(_lr))\n",
    "  else:\n",
    "    print(\"Improper modality provided!\")\n",
    "\n",
    "  return train_loss, train_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CXy5mkXruLRr"
   },
   "outputs": [],
   "source": [
    "# Define modalities\n",
    "# _modality = ['visual', 'audio', 'multimodal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "eaq2Nfvs9IZm"
   },
   "outputs": [],
   "source": [
    "# Train ViT\n",
    "# scores = {}\n",
    "# for _m in _modality:\n",
    "#   train_loss, train_accuracy, test_loss, test_accuracy = train_ViT(_m)\n",
    "#   scores[_m] = [train_loss, train_accuracy, test_loss, test_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "XNOX2sOP0E3j"
   },
   "outputs": [],
   "source": [
    "# # Print results\n",
    "# print(\"\\nResults\\n---------------\\n\")\n",
    "# for _m, val in scores.items():\n",
    "#   print(f\"Modality: {_m}, Train Loss: {val[0]:.4f}, Train Accuracy: {val[1]:.4f}, Test Loss: {val[2]:.4f}, Test Accuracy: {val[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7eQaDTdZ9Qak"
   },
   "outputs": [],
   "source": [
    "# # Copy trained models to GDrive\n",
    "# !cp 'ViT_audio_video_fullscale_50_16_0.0001' '/content/drive/MyDrive/csci535/models'\n",
    "# !cp 'ViT_audio_fullscale_50_16_0.0001' '/content/drive/MyDrive/csci535/models'\n",
    "# !cp 'ViT_video_fullscale_50_16_0.0001' '/content/drive/MyDrive/csci535/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Gaxvcn5h7J33"
   },
   "outputs": [],
   "source": [
    "def cross_test_ViT(_modality_model, _modality_features):\n",
    "  _input_channels = 1\n",
    "  model = ViT(img_dim=224,  # Image dimension\n",
    "              in_channels=_input_channels,  # Number of input channels\n",
    "              patch_dim=16,  # Patch dimension\n",
    "              num_classes=6,  # 6 classes for HAPPY, SAD, ANGRY, DISGUST, FEAR, NEUTRAL\n",
    "              dim=768,  # Dimensionality of the token embeddings\n",
    "              blocks=16,  # Number of transformer blocks\n",
    "              heads=16,  # Number of attention heads\n",
    "              dim_linear_block=1024,  # Dimensionality of the linear block\n",
    "              dropout=0.3,  # Dropout rate\n",
    "              classification=True)  # Whether or not to include a classification token\n",
    "\n",
    "  if _modality_model == 'multimodal':\n",
    "      state_dict = torch.load(f'/home1/riyaranj/riya/ViT_audio_video_fullscale__64_16_16_0.3_0.0001')\n",
    "  else:\n",
    "      state_dict = torch.load(f'/home1/riyaranj/riya/ViT_{_modality_model}_fullscale__64_16_16_0.3_0.0001')\n",
    "    #state_dict = torch.load('/home1/riyaranj/riya/ViT_' + _modality_model + '_fullscale__32_8_8_0.2_0.0001')\n",
    "  model.load_state_dict(state_dict)\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model.to(device)\n",
    "  # Define loss function and optimizer\n",
    "  _lr = 0.0001\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=_lr)\n",
    "\n",
    "  test_dataset = ConcatDataset(X_test, X_test_spec, y_test, modality = _modality_features, presaved = _presaved)\n",
    "\n",
    "  # Create data loaders\n",
    "  _bs = 64\n",
    "\n",
    "  # Create data loaders\n",
    "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=_bs)\n",
    "  print(f\"\\nTesting {_modality_model}-trained ViT pipeline on {_modality_features} features ...\\n---------------------------------------------------------\\n\")\n",
    "  test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
    "  print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypSMQZwN7Q4p",
    "outputId": "db70adee-8661-4074-de87-bcbcffc3494d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing audio-trained ViT pipeline on visual features ...\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:22<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.8865, Test Accuracy: 0.1772\n",
      "\n",
      "Testing video-trained ViT pipeline on audio features ...\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:10<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.5673, Test Accuracy: 0.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Peform cross-test\n",
    "cross_test_ViT('audio', 'visual')\n",
    "cross_test_ViT('video', 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBExU4XL7RVh",
    "outputId": "e1a9f3c4-081e-46c5-e2c2-b1eb5ebab478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing audio-trained ViT pipeline on audio features ...\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:10<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3624, Test Accuracy: 0.4477\n",
      "\n",
      "Testing video-trained ViT pipeline on visual features ...\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:10<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4643, Test Accuracy: 0.5361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform straight-test for sanity check\n",
    "cross_test_ViT('audio', 'audio')\n",
    "cross_test_ViT('video', 'visual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XzFnQhXhmCiI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin\n"
     ]
    }
   ],
   "source": [
    "print(\"Fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12.2 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
